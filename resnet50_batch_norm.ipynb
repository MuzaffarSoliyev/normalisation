{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet50_batch_norm.ipynb","provenance":[],"authorship_tag":"ABX9TyND1Cs/2VKaqiyjftsVYsJ3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6c519aceae204764b87ef380c4820c51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7bcb6f891e7046ff9eebada73230db86","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7833fbc1339b409d91a04b3917d41fff","IPY_MODEL_1d67d115a6a44c1eaba2091672a29685"]}},"7bcb6f891e7046ff9eebada73230db86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7833fbc1339b409d91a04b3917d41fff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8f8989e8280147408c0a9e121ddcfdd7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_05220eac65984e409cc6bdb6db9f580d"}},"1d67d115a6a44c1eaba2091672a29685":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_08032b254d8747f0a0c659f306d1097f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169009152/? [00:20&lt;00:00, 33669508.14it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_06b8b03a51014b50917f18f2a850427a"}},"8f8989e8280147408c0a9e121ddcfdd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"05220eac65984e409cc6bdb6db9f580d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08032b254d8747f0a0c659f306d1097f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"06b8b03a51014b50917f18f2a850427a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c96bc02f37244b10932479c2ed7b1f95":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1ce3b35cca13442885eb0aa698744c86","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e80314e2d5b3487eacf9f245f37f8191","IPY_MODEL_804ceefa512f44dca7aeccf4d9e6be72"]}},"1ce3b35cca13442885eb0aa698744c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e80314e2d5b3487eacf9f245f37f8191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_982fc80162074526b1ee63f42821713d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b92f5861028a4fd58c6aaa2417d37434"}},"804ceefa512f44dca7aeccf4d9e6be72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e08c249e24704d798a0536d077c12e12","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169009152/? [00:20&lt;00:00, 33575727.16it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7caa48f149e4d85aeee252e37117e67"}},"982fc80162074526b1ee63f42821713d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b92f5861028a4fd58c6aaa2417d37434":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e08c249e24704d798a0536d077c12e12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7caa48f149e4d85aeee252e37117e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"YGRorx9rlWvK","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torchvision as tv\n","import torch.optim as O"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSjAVod1lfva","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHrLQzPslhk3","colab_type":"code","outputId":"b9a22252-3666-4152-dd96-2a27c7e22b43","executionInfo":{"status":"ok","timestamp":1590605793347,"user_tz":-300,"elapsed":617,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print(torch.cuda.is_available())\n","print(torch.cuda.current_device())\n","print(torch.cuda.device(0))\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["True\n","0\n","<torch.cuda.device object at 0x7f020d900da0>\n","1\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3vizPiK1lj03","colab_type":"code","outputId":"0ae4bdc4-392c-469f-f4e1-906230634a1a","executionInfo":{"status":"ok","timestamp":1590605796604,"user_tz":-300,"elapsed":1046,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using device: cuda\n","\n","Tesla P100-PCIE-16GB\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w_wv5i5OlmVn","colab_type":"code","colab":{}},"source":["import torch\n","import random\n","import numpy as np\n","\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.backends.cudnn.deterministic = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWVkJnNFloH_","colab_type":"code","colab":{}},"source":["cifar100 = tv.datasets.CIFAR100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKh9rjUxlqT_","colab_type":"code","colab":{}},"source":["train_batch = 256\n","test_batch = 256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RlbbVuDPlswn","colab_type":"code","colab":{}},"source":["import torchvision.transforms as transforms\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343], std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])\n","])\n","\n","# Normalize test set same as training set without augmentation\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343], std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8koZBV3EmFRt","colab_type":"code","outputId":"57cd97f5-29be-4c31-e68e-6031caaf21e7","executionInfo":{"status":"ok","timestamp":1590605825236,"user_tz":-300,"elapsed":18990,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}},"colab":{"base_uri":"https://localhost:8080/","height":149,"referenced_widgets":["6c519aceae204764b87ef380c4820c51","7bcb6f891e7046ff9eebada73230db86","7833fbc1339b409d91a04b3917d41fff","1d67d115a6a44c1eaba2091672a29685","8f8989e8280147408c0a9e121ddcfdd7","05220eac65984e409cc6bdb6db9f580d","08032b254d8747f0a0c659f306d1097f","06b8b03a51014b50917f18f2a850427a","c96bc02f37244b10932479c2ed7b1f95","1ce3b35cca13442885eb0aa698744c86","e80314e2d5b3487eacf9f245f37f8191","804ceefa512f44dca7aeccf4d9e6be72","982fc80162074526b1ee63f42821713d","b92f5861028a4fd58c6aaa2417d37434","e08c249e24704d798a0536d077c12e12","b7caa48f149e4d85aeee252e37117e67"]}},"source":["trainset = tv.datasets.CIFAR100(root='./cifar100/train',\n","                                         train=True,\n","                                         download=True,\n","                                         transform=transform_train)\n","cifar100_train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=train_batch, shuffle=True, num_workers=4)\n","\n","testset = tv.datasets.CIFAR100(root='./cifar100/test',\n","                                        train=False,\n","                                        download=True,\n","                                        transform=transform_test)\n","cifar100_test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=test_batch, shuffle=False, num_workers=4)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/train/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c519aceae204764b87ef380c4820c51","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./cifar100/train/cifar-100-python.tar.gz to ./cifar100/train\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/test/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c96bc02f37244b10932479c2ed7b1f95","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./cifar100/test/cifar-100-python.tar.gz to ./cifar100/test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztWu5UktmHYN","colab_type":"code","colab":{}},"source":["def conv3x3(in_channels, out_channels, stride=1):\n","    \"\"\"3x3 kernel size with padding convolutional layer in ResNet BasicBlock.\"\"\"\n","    return nn.Conv2d(\n","        in_channels=in_channels,\n","        out_channels=out_channels,\n","        kernel_size=3,\n","        stride=stride,\n","        padding=1,\n","        bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"Basic Block of ReseNet.\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        \"\"\"Basic Block of ReseNet Builder.\"\"\"\n","        super(BasicBlock, self).__init__()\n","\n","        # First conv3x3 layer\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","\n","        #  Batch Normalization\n","        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n","\n","        # ReLU Activation Function\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # Second conv3x3 layer\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","\n","        #  Batch Normalization\n","        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n","\n","        # downsample for `residual`\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        \"\"\"Forward Pass of Basic Block.\"\"\"\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        return out\n","\n","class ResNet(nn.Module):\n","    \"\"\"Residual Neural Network.\"\"\"\n","\n","    def __init__(self, block, duplicates, num_classes=100):\n","        \"\"\"Residual Neural Network Builder.\"\"\"\n","        super(ResNet, self).__init__()\n","\n","        self.in_channels = 32\n","        self.conv1 = conv3x3(in_channels=3, out_channels=32)\n","        self.bn = nn.BatchNorm2d(num_features=32)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout2d(p=0.02)\n","\n","        # block of Basic Blocks\n","        self.conv2_x = self._make_block(block, duplicates[0], out_channels=32)\n","        self.conv3_x = self._make_block(block, duplicates[1], out_channels=64, stride=2)\n","        self.conv4_x = self._make_block(block, duplicates[2], out_channels=128, stride=2)\n","        self.conv5_x = self._make_block(block, duplicates[3], out_channels=256, stride=2)\n","\n","        self.maxpool = nn.MaxPool2d(kernel_size=4, stride=1)\n","        self.fc_layer = nn.Linear(256, num_classes)\n","\n","        # initialize weights\n","        # self.apply(initialize_weights)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.bias.data.zero_()\n","\n","    def _make_block(self, block, duplicates, out_channels, stride=1):\n","        \"\"\"\n","        Create Block in ResNet.\n","\n","        Args:\n","            block: BasicBlock\n","            duplicates: number of BasicBlock\n","            out_channels: out channels of the block\n","\n","        Returns:\n","            nn.Sequential(*layers)\n","        \"\"\"\n","        downsample = None\n","        if (stride != 1) or (self.in_channels != out_channels):\n","            downsample = nn.Sequential(\n","                conv3x3(self.in_channels, out_channels, stride=stride),\n","                nn.BatchNorm2d(num_features=out_channels)\n","            )\n","\n","        layers = []\n","        layers.append(\n","            block(self.in_channels, out_channels, stride, downsample))\n","        self.in_channels = out_channels\n","        for _ in range(1, duplicates):\n","            layers.append(block(out_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass of ResNet.\"\"\"\n","        out = self.conv1(x)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","\n","        # Stacked Basic Blocks\n","        out = self.conv2_x(out)\n","        out = self.conv3_x(out)\n","        out = self.conv4_x(out)\n","        out = self.conv5_x(out)\n","\n","        out = self.maxpool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc_layer(out)\n","\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TejkVZLRmJM1","colab_type":"code","outputId":"b8eca79b-44ac-4c14-ebed-730499f0251f","executionInfo":{"status":"ok","timestamp":1590605849731,"user_tz":-300,"elapsed":1051,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lenet5_cifar100 = ResNet(BasicBlock, [3, 4, 6, 3])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oi-4n4jCmKcu","colab_type":"code","colab":{}},"source":["validate_every = 2000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFGAOj5lmL11","colab_type":"code","outputId":"a60e3e66-ab8f-42c3-95bb-97a4f1c14962","executionInfo":{"status":"error","timestamp":1590609983331,"user_tz":-300,"elapsed":4130373,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["lenet5_cifar100_dev = lenet5_cifar100.to(device)\n","opt = O.SGD(lenet5_cifar100.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n","scheduler = O.lr_scheduler.MultiStepLR(opt, milestones=[70, 120],gamma=0.1)\n","criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n","train_cross_entropy = []\n","train_accuracy = []\n","validation_cross_entropy = []\n","validation_accuracy = []\n","\n","best_model_accuracy = 0\n","\n","for epoch in range(500):\n","    n_correct = 0\n","    n_total = 0\n","    for i, batch in enumerate(cifar100_train_loader):\n","        x, labels = batch\n","        x, labels = x.to(device), labels.to(device)\n","        N = x.shape[0]\n","        \n","        # training mode (for things like dropout)\n","        lenet5_cifar100_dev.train()\n","        \n","        # clear previous gradients\n","        opt.zero_grad()\n","        \n","        y_hat = lenet5_cifar100_dev(x)\n","        loss = criterion(y_hat, labels)\n","        loss.backward()\n","        opt.step()\n","        \n","        train_cross_entropy.append(loss)\n","        \n","        n_correct += (torch.argmax(y_hat, dim=1) == labels).sum().item()\n","        n_total += N\n","        \n","        # evaluation mode (e.g. adds dropped neurons back in)\n","        lenet5_cifar100_dev.eval()\n","        if i % validate_every == 0:\n","            n_val_correct = 0\n","            n_val_total = 0\n","            v_cross_entropy_sum = 0\n","            \n","            # don't calculate gradients here\n","            with torch.no_grad():\n","                for j, v_batch in enumerate(cifar100_test_loader):\n","                    v_x, v_labels = v_batch\n","                    v_x, v_labels = v_x.to(device), v_labels.to(device)\n","                    v_N = v_x.shape[0]\n","                    \n","                    v_y_hat = lenet5_cifar100_dev(v_x)\n","                    v_loss = criterion(v_y_hat, v_labels)\n","                    v_cross_entropy_sum += v_loss\n","                    n_val_correct += (torch.argmax(v_y_hat, dim=1) == v_labels).sum().item()\n","                    n_val_total += v_N\n","\n","            print(f\"[epoch {epoch + 1}, iteration {i}] \\t accuracy: {n_val_correct / n_val_total} \\t cross entropy: {v_cross_entropy_sum / n_val_total}\")\n","            validation_accuracy.append(n_val_correct / n_val_total)\n","            validation_cross_entropy.append(v_cross_entropy_sum / n_val_total)\n","            if n_val_correct / n_val_total >= best_model_accuracy:\n","                best_model_accuracy = n_val_correct / n_val_total\n","                print(\"saving\")\n","                torch.save(lenet5_cifar100_dev.state_dict(), './lenet5_cifar100')\n","    \n","    print(f\"epoch {epoch + 1} accumulated train accuracy: {n_correct / n_total}\")\n","    train_accuracy.append(n_correct / n_total)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[epoch 1, iteration 0] \t accuracy: 0.01 \t cross entropy: 0.7415557503700256\n","saving\n","epoch 1 accumulated train accuracy: 0.08738\n","[epoch 2, iteration 0] \t accuracy: 0.1318 \t cross entropy: 0.014704286120831966\n","saving\n","epoch 2 accumulated train accuracy: 0.16596\n","[epoch 3, iteration 0] \t accuracy: 0.2066 \t cross entropy: 0.013347195461392403\n","saving\n","epoch 3 accumulated train accuracy: 0.22384\n","[epoch 4, iteration 0] \t accuracy: 0.2295 \t cross entropy: 0.012478957884013653\n","saving\n","epoch 4 accumulated train accuracy: 0.2665\n","[epoch 5, iteration 0] \t accuracy: 0.2886 \t cross entropy: 0.011491993442177773\n","saving\n","epoch 5 accumulated train accuracy: 0.30984\n","[epoch 6, iteration 0] \t accuracy: 0.3266 \t cross entropy: 0.01053973101079464\n","saving\n","epoch 6 accumulated train accuracy: 0.34982\n","[epoch 7, iteration 0] \t accuracy: 0.3505 \t cross entropy: 0.010010012425482273\n","saving\n","epoch 7 accumulated train accuracy: 0.38416\n","[epoch 8, iteration 0] \t accuracy: 0.385 \t cross entropy: 0.009487569332122803\n","saving\n","epoch 8 accumulated train accuracy: 0.41884\n","[epoch 9, iteration 0] \t accuracy: 0.4105 \t cross entropy: 0.008941328153014183\n","saving\n","epoch 9 accumulated train accuracy: 0.44934\n","[epoch 10, iteration 0] \t accuracy: 0.4344 \t cross entropy: 0.008582022041082382\n","saving\n","epoch 10 accumulated train accuracy: 0.4743\n","[epoch 11, iteration 0] \t accuracy: 0.4498 \t cross entropy: 0.008290722034871578\n","saving\n","epoch 11 accumulated train accuracy: 0.49682\n","[epoch 12, iteration 0] \t accuracy: 0.4776 \t cross entropy: 0.007771869655698538\n","saving\n","epoch 12 accumulated train accuracy: 0.51688\n","[epoch 13, iteration 0] \t accuracy: 0.4888 \t cross entropy: 0.007639956660568714\n","saving\n","epoch 13 accumulated train accuracy: 0.53654\n","[epoch 14, iteration 0] \t accuracy: 0.5075 \t cross entropy: 0.007430005352944136\n","saving\n","epoch 14 accumulated train accuracy: 0.5573\n","[epoch 15, iteration 0] \t accuracy: 0.5005 \t cross entropy: 0.0074141668155789375\n","epoch 15 accumulated train accuracy: 0.56976\n","[epoch 16, iteration 0] \t accuracy: 0.5275 \t cross entropy: 0.007076176814734936\n","saving\n","epoch 16 accumulated train accuracy: 0.58708\n","[epoch 17, iteration 0] \t accuracy: 0.5351 \t cross entropy: 0.006904686335474253\n","saving\n","epoch 17 accumulated train accuracy: 0.60436\n","[epoch 18, iteration 0] \t accuracy: 0.5334 \t cross entropy: 0.007104289252310991\n","epoch 18 accumulated train accuracy: 0.61494\n","[epoch 19, iteration 0] \t accuracy: 0.541 \t cross entropy: 0.006990663707256317\n","saving\n","epoch 19 accumulated train accuracy: 0.63072\n","[epoch 20, iteration 0] \t accuracy: 0.5611 \t cross entropy: 0.006673735566437244\n","saving\n","epoch 20 accumulated train accuracy: 0.64424\n","[epoch 21, iteration 0] \t accuracy: 0.5518 \t cross entropy: 0.006841123569756746\n","epoch 21 accumulated train accuracy: 0.65316\n","[epoch 22, iteration 0] \t accuracy: 0.5639 \t cross entropy: 0.006693059578537941\n","saving\n","epoch 22 accumulated train accuracy: 0.66672\n","[epoch 23, iteration 0] \t accuracy: 0.5719 \t cross entropy: 0.0063687521032989025\n","saving\n","epoch 23 accumulated train accuracy: 0.67754\n","[epoch 24, iteration 0] \t accuracy: 0.5634 \t cross entropy: 0.006767513230443001\n","epoch 24 accumulated train accuracy: 0.6863\n","[epoch 25, iteration 0] \t accuracy: 0.5795 \t cross entropy: 0.006548995152115822\n","saving\n","epoch 25 accumulated train accuracy: 0.6956\n","[epoch 26, iteration 0] \t accuracy: 0.5791 \t cross entropy: 0.0064377873204648495\n","epoch 26 accumulated train accuracy: 0.70712\n","[epoch 27, iteration 0] \t accuracy: 0.5795 \t cross entropy: 0.006490515545010567\n","saving\n","epoch 27 accumulated train accuracy: 0.71504\n","[epoch 28, iteration 0] \t accuracy: 0.5893 \t cross entropy: 0.006447183899581432\n","saving\n","epoch 28 accumulated train accuracy: 0.72786\n","[epoch 29, iteration 0] \t accuracy: 0.5779 \t cross entropy: 0.006742130033671856\n","epoch 29 accumulated train accuracy: 0.7377\n","[epoch 30, iteration 0] \t accuracy: 0.5981 \t cross entropy: 0.006284693721681833\n","saving\n","epoch 30 accumulated train accuracy: 0.74464\n","[epoch 31, iteration 0] \t accuracy: 0.5954 \t cross entropy: 0.006440218072384596\n","epoch 31 accumulated train accuracy: 0.75262\n","[epoch 32, iteration 0] \t accuracy: 0.6016 \t cross entropy: 0.00652939872816205\n","saving\n","epoch 32 accumulated train accuracy: 0.76618\n","[epoch 33, iteration 0] \t accuracy: 0.6026 \t cross entropy: 0.006447847466915846\n","saving\n","epoch 33 accumulated train accuracy: 0.76926\n","[epoch 34, iteration 0] \t accuracy: 0.5985 \t cross entropy: 0.006616206839680672\n","epoch 34 accumulated train accuracy: 0.7772\n","[epoch 35, iteration 0] \t accuracy: 0.6112 \t cross entropy: 0.0064712753519415855\n","saving\n","epoch 35 accumulated train accuracy: 0.78574\n","[epoch 36, iteration 0] \t accuracy: 0.6026 \t cross entropy: 0.006561296060681343\n","epoch 36 accumulated train accuracy: 0.79502\n","[epoch 37, iteration 0] \t accuracy: 0.6093 \t cross entropy: 0.006537767592817545\n","epoch 37 accumulated train accuracy: 0.8051\n","[epoch 38, iteration 0] \t accuracy: 0.6096 \t cross entropy: 0.006809072569012642\n","epoch 38 accumulated train accuracy: 0.81008\n","[epoch 39, iteration 0] \t accuracy: 0.6034 \t cross entropy: 0.0069847130216658115\n","epoch 39 accumulated train accuracy: 0.8157\n","[epoch 40, iteration 0] \t accuracy: 0.608 \t cross entropy: 0.006802381947636604\n","epoch 40 accumulated train accuracy: 0.82742\n","[epoch 41, iteration 0] \t accuracy: 0.6071 \t cross entropy: 0.006945849396288395\n","epoch 41 accumulated train accuracy: 0.8303\n","[epoch 42, iteration 0] \t accuracy: 0.6163 \t cross entropy: 0.0067338477820158005\n","saving\n","epoch 42 accumulated train accuracy: 0.84134\n","[epoch 43, iteration 0] \t accuracy: 0.5989 \t cross entropy: 0.00705730402842164\n","epoch 43 accumulated train accuracy: 0.84764\n","[epoch 44, iteration 0] \t accuracy: 0.6166 \t cross entropy: 0.007043121382594109\n","saving\n","epoch 44 accumulated train accuracy: 0.85108\n","[epoch 45, iteration 0] \t accuracy: 0.6246 \t cross entropy: 0.006655065808445215\n","saving\n","epoch 45 accumulated train accuracy: 0.85772\n","[epoch 46, iteration 0] \t accuracy: 0.6196 \t cross entropy: 0.006994853727519512\n","epoch 46 accumulated train accuracy: 0.86248\n","[epoch 47, iteration 0] \t accuracy: 0.6193 \t cross entropy: 0.007059098687022924\n","epoch 47 accumulated train accuracy: 0.86666\n","[epoch 48, iteration 0] \t accuracy: 0.619 \t cross entropy: 0.007113876286894083\n","epoch 48 accumulated train accuracy: 0.87626\n","[epoch 49, iteration 0] \t accuracy: 0.615 \t cross entropy: 0.0072074392810463905\n","epoch 49 accumulated train accuracy: 0.87902\n","[epoch 50, iteration 0] \t accuracy: 0.6075 \t cross entropy: 0.007618015166372061\n","epoch 50 accumulated train accuracy: 0.88504\n","[epoch 51, iteration 0] \t accuracy: 0.6266 \t cross entropy: 0.007098052185028791\n","saving\n","epoch 51 accumulated train accuracy: 0.88948\n","[epoch 52, iteration 0] \t accuracy: 0.622 \t cross entropy: 0.007210238371044397\n","epoch 52 accumulated train accuracy: 0.89452\n","[epoch 53, iteration 0] \t accuracy: 0.6194 \t cross entropy: 0.007609195541590452\n","epoch 53 accumulated train accuracy: 0.8994\n","[epoch 54, iteration 0] \t accuracy: 0.627 \t cross entropy: 0.007291065063327551\n","saving\n","epoch 54 accumulated train accuracy: 0.90256\n","[epoch 55, iteration 0] \t accuracy: 0.6259 \t cross entropy: 0.007476945873349905\n","epoch 55 accumulated train accuracy: 0.90634\n","[epoch 56, iteration 0] \t accuracy: 0.6239 \t cross entropy: 0.0076386649161577225\n","epoch 56 accumulated train accuracy: 0.9144\n","[epoch 57, iteration 0] \t accuracy: 0.623 \t cross entropy: 0.007455905899405479\n","epoch 57 accumulated train accuracy: 0.91468\n","[epoch 58, iteration 0] \t accuracy: 0.6249 \t cross entropy: 0.007579723373055458\n","epoch 58 accumulated train accuracy: 0.91668\n","[epoch 59, iteration 0] \t accuracy: 0.6292 \t cross entropy: 0.00757230119779706\n","saving\n","epoch 59 accumulated train accuracy: 0.9206\n","[epoch 60, iteration 0] \t accuracy: 0.6277 \t cross entropy: 0.007757582701742649\n","epoch 60 accumulated train accuracy: 0.92678\n","[epoch 61, iteration 0] \t accuracy: 0.6373 \t cross entropy: 0.007431225385516882\n","saving\n","epoch 61 accumulated train accuracy: 0.93138\n","[epoch 62, iteration 0] \t accuracy: 0.6312 \t cross entropy: 0.00759593490511179\n","epoch 62 accumulated train accuracy: 0.93162\n","[epoch 63, iteration 0] \t accuracy: 0.6407 \t cross entropy: 0.007632715627551079\n","saving\n","epoch 63 accumulated train accuracy: 0.9368\n","[epoch 64, iteration 0] \t accuracy: 0.6242 \t cross entropy: 0.007935615256428719\n","epoch 64 accumulated train accuracy: 0.94022\n","[epoch 65, iteration 0] \t accuracy: 0.6348 \t cross entropy: 0.007886391133069992\n","epoch 65 accumulated train accuracy: 0.93942\n","[epoch 66, iteration 0] \t accuracy: 0.633 \t cross entropy: 0.007926307618618011\n","epoch 66 accumulated train accuracy: 0.94326\n","[epoch 67, iteration 0] \t accuracy: 0.6341 \t cross entropy: 0.0077157714404165745\n","epoch 67 accumulated train accuracy: 0.94658\n","[epoch 68, iteration 0] \t accuracy: 0.6406 \t cross entropy: 0.007808533497154713\n","epoch 68 accumulated train accuracy: 0.94636\n","[epoch 69, iteration 0] \t accuracy: 0.6299 \t cross entropy: 0.008040846325457096\n","epoch 69 accumulated train accuracy: 0.94972\n","[epoch 70, iteration 0] \t accuracy: 0.6332 \t cross entropy: 0.008131432346999645\n","epoch 70 accumulated train accuracy: 0.95208\n","[epoch 71, iteration 0] \t accuracy: 0.6382 \t cross entropy: 0.00819004699587822\n","epoch 71 accumulated train accuracy: 0.95124\n","[epoch 72, iteration 0] \t accuracy: 0.6354 \t cross entropy: 0.008147305808961391\n","epoch 72 accumulated train accuracy: 0.95472\n","[epoch 73, iteration 0] \t accuracy: 0.6386 \t cross entropy: 0.008084663189947605\n","epoch 73 accumulated train accuracy: 0.95742\n","[epoch 74, iteration 0] \t accuracy: 0.6413 \t cross entropy: 0.008088716305792332\n","saving\n","epoch 74 accumulated train accuracy: 0.95956\n","[epoch 75, iteration 0] \t accuracy: 0.6387 \t cross entropy: 0.008321987465023994\n","epoch 75 accumulated train accuracy: 0.95932\n","[epoch 76, iteration 0] \t accuracy: 0.6422 \t cross entropy: 0.008238342590630054\n","saving\n","epoch 76 accumulated train accuracy: 0.9613\n","[epoch 77, iteration 0] \t accuracy: 0.6386 \t cross entropy: 0.008269540034234524\n","epoch 77 accumulated train accuracy: 0.96068\n","[epoch 78, iteration 0] \t accuracy: 0.6398 \t cross entropy: 0.00824593473225832\n","epoch 78 accumulated train accuracy: 0.96554\n","[epoch 79, iteration 0] \t accuracy: 0.6364 \t cross entropy: 0.008256401866674423\n","epoch 79 accumulated train accuracy: 0.9661\n","[epoch 80, iteration 0] \t accuracy: 0.6464 \t cross entropy: 0.0082083223387599\n","saving\n","epoch 80 accumulated train accuracy: 0.96814\n","[epoch 81, iteration 0] \t accuracy: 0.6383 \t cross entropy: 0.008503294549882412\n","epoch 81 accumulated train accuracy: 0.96912\n","[epoch 82, iteration 0] \t accuracy: 0.6461 \t cross entropy: 0.008374350145459175\n","epoch 82 accumulated train accuracy: 0.96996\n","[epoch 83, iteration 0] \t accuracy: 0.6435 \t cross entropy: 0.008310746401548386\n","epoch 83 accumulated train accuracy: 0.97204\n","[epoch 84, iteration 0] \t accuracy: 0.6396 \t cross entropy: 0.008457382209599018\n","epoch 84 accumulated train accuracy: 0.97294\n","[epoch 85, iteration 0] \t accuracy: 0.6484 \t cross entropy: 0.008268130011856556\n","saving\n","epoch 85 accumulated train accuracy: 0.97344\n","[epoch 86, iteration 0] \t accuracy: 0.6453 \t cross entropy: 0.00840786099433899\n","epoch 86 accumulated train accuracy: 0.9709\n","[epoch 87, iteration 0] \t accuracy: 0.6484 \t cross entropy: 0.008435563184320927\n","saving\n","epoch 87 accumulated train accuracy: 0.97564\n","[epoch 88, iteration 0] \t accuracy: 0.6472 \t cross entropy: 0.008350970223546028\n","epoch 88 accumulated train accuracy: 0.97776\n","[epoch 89, iteration 0] \t accuracy: 0.6435 \t cross entropy: 0.00858615804463625\n","epoch 89 accumulated train accuracy: 0.97664\n","[epoch 90, iteration 0] \t accuracy: 0.6463 \t cross entropy: 0.008494398556649685\n","epoch 90 accumulated train accuracy: 0.97598\n","[epoch 91, iteration 0] \t accuracy: 0.6482 \t cross entropy: 0.008339582942426205\n","epoch 91 accumulated train accuracy: 0.97738\n","[epoch 92, iteration 0] \t accuracy: 0.6489 \t cross entropy: 0.008456484414637089\n","saving\n","epoch 92 accumulated train accuracy: 0.97792\n","[epoch 93, iteration 0] \t accuracy: 0.645 \t cross entropy: 0.008621742017567158\n","epoch 93 accumulated train accuracy: 0.98054\n","[epoch 94, iteration 0] \t accuracy: 0.6484 \t cross entropy: 0.008448103442788124\n","epoch 94 accumulated train accuracy: 0.98218\n","[epoch 95, iteration 0] \t accuracy: 0.6561 \t cross entropy: 0.008642545901238918\n","saving\n","epoch 95 accumulated train accuracy: 0.98204\n","[epoch 96, iteration 0] \t accuracy: 0.6548 \t cross entropy: 0.00855790264904499\n","epoch 96 accumulated train accuracy: 0.9829\n","[epoch 97, iteration 0] \t accuracy: 0.6551 \t cross entropy: 0.008542779833078384\n","epoch 97 accumulated train accuracy: 0.98324\n","[epoch 98, iteration 0] \t accuracy: 0.6532 \t cross entropy: 0.008688035421073437\n","epoch 98 accumulated train accuracy: 0.9825\n","[epoch 99, iteration 0] \t accuracy: 0.6512 \t cross entropy: 0.008556446060538292\n","epoch 99 accumulated train accuracy: 0.98172\n","[epoch 100, iteration 0] \t accuracy: 0.6562 \t cross entropy: 0.00877594668418169\n","saving\n","epoch 100 accumulated train accuracy: 0.9836\n","[epoch 101, iteration 0] \t accuracy: 0.6521 \t cross entropy: 0.0086814034730196\n","epoch 101 accumulated train accuracy: 0.98332\n","[epoch 102, iteration 0] \t accuracy: 0.6477 \t cross entropy: 0.008921939879655838\n","epoch 102 accumulated train accuracy: 0.9833\n","[epoch 103, iteration 0] \t accuracy: 0.654 \t cross entropy: 0.008663798682391644\n","epoch 103 accumulated train accuracy: 0.98392\n","[epoch 104, iteration 0] \t accuracy: 0.6554 \t cross entropy: 0.008802075870335102\n","epoch 104 accumulated train accuracy: 0.98672\n","[epoch 105, iteration 0] \t accuracy: 0.6622 \t cross entropy: 0.008706898428499699\n","saving\n","epoch 105 accumulated train accuracy: 0.98706\n","[epoch 106, iteration 0] \t accuracy: 0.6576 \t cross entropy: 0.008994242176413536\n","epoch 106 accumulated train accuracy: 0.9858\n","[epoch 107, iteration 0] \t accuracy: 0.6516 \t cross entropy: 0.008898709900677204\n","epoch 107 accumulated train accuracy: 0.98496\n","[epoch 108, iteration 0] \t accuracy: 0.6536 \t cross entropy: 0.008943576365709305\n","epoch 108 accumulated train accuracy: 0.98664\n","[epoch 109, iteration 0] \t accuracy: 0.6607 \t cross entropy: 0.00860154815018177\n","epoch 109 accumulated train accuracy: 0.98832\n","[epoch 110, iteration 0] \t accuracy: 0.6586 \t cross entropy: 0.008779277093708515\n","epoch 110 accumulated train accuracy: 0.98842\n","[epoch 111, iteration 0] \t accuracy: 0.6598 \t cross entropy: 0.008837519213557243\n","epoch 111 accumulated train accuracy: 0.98696\n","[epoch 112, iteration 0] \t accuracy: 0.6604 \t cross entropy: 0.008916999213397503\n","epoch 112 accumulated train accuracy: 0.98832\n","[epoch 113, iteration 0] \t accuracy: 0.6556 \t cross entropy: 0.00904027372598648\n","epoch 113 accumulated train accuracy: 0.98916\n","[epoch 114, iteration 0] \t accuracy: 0.6547 \t cross entropy: 0.009043467231094837\n","epoch 114 accumulated train accuracy: 0.99096\n","[epoch 115, iteration 0] \t accuracy: 0.6567 \t cross entropy: 0.00882807094603777\n","epoch 115 accumulated train accuracy: 0.99016\n","[epoch 116, iteration 0] \t accuracy: 0.6653 \t cross entropy: 0.008780770003795624\n","saving\n","epoch 116 accumulated train accuracy: 0.98834\n","[epoch 117, iteration 0] \t accuracy: 0.6583 \t cross entropy: 0.00893584918230772\n","epoch 117 accumulated train accuracy: 0.98924\n","[epoch 118, iteration 0] \t accuracy: 0.6594 \t cross entropy: 0.008988372050225735\n","epoch 118 accumulated train accuracy: 0.98792\n","[epoch 119, iteration 0] \t accuracy: 0.6607 \t cross entropy: 0.008888797834515572\n","epoch 119 accumulated train accuracy: 0.99096\n","[epoch 120, iteration 0] \t accuracy: 0.6589 \t cross entropy: 0.008899634703993797\n","epoch 120 accumulated train accuracy: 0.9909\n","[epoch 121, iteration 0] \t accuracy: 0.6607 \t cross entropy: 0.008863335475325584\n","epoch 121 accumulated train accuracy: 0.9913\n","[epoch 122, iteration 0] \t accuracy: 0.6606 \t cross entropy: 0.008950618095695972\n","epoch 122 accumulated train accuracy: 0.98936\n","[epoch 123, iteration 0] \t accuracy: 0.6623 \t cross entropy: 0.008968581445515156\n","epoch 123 accumulated train accuracy: 0.99094\n","[epoch 124, iteration 0] \t accuracy: 0.6563 \t cross entropy: 0.009036594070494175\n","epoch 124 accumulated train accuracy: 0.99078\n","[epoch 125, iteration 0] \t accuracy: 0.6599 \t cross entropy: 0.009072797372937202\n","epoch 125 accumulated train accuracy: 0.9914\n","[epoch 126, iteration 0] \t accuracy: 0.6592 \t cross entropy: 0.009004462510347366\n","epoch 126 accumulated train accuracy: 0.99066\n","[epoch 127, iteration 0] \t accuracy: 0.6599 \t cross entropy: 0.008998686447739601\n","epoch 127 accumulated train accuracy: 0.9911\n","[epoch 128, iteration 0] \t accuracy: 0.6589 \t cross entropy: 0.009084677323698997\n","epoch 128 accumulated train accuracy: 0.99166\n","[epoch 129, iteration 0] \t accuracy: 0.6604 \t cross entropy: 0.009075881913304329\n","epoch 129 accumulated train accuracy: 0.9918\n","[epoch 130, iteration 0] \t accuracy: 0.6609 \t cross entropy: 0.00912515353411436\n","epoch 130 accumulated train accuracy: 0.99116\n","[epoch 131, iteration 0] \t accuracy: 0.6633 \t cross entropy: 0.00916684977710247\n","epoch 131 accumulated train accuracy: 0.99282\n","[epoch 132, iteration 0] \t accuracy: 0.6631 \t cross entropy: 0.009048049338161945\n","epoch 132 accumulated train accuracy: 0.99314\n","[epoch 133, iteration 0] \t accuracy: 0.6638 \t cross entropy: 0.00900198519229889\n","epoch 133 accumulated train accuracy: 0.99294\n","[epoch 134, iteration 0] \t accuracy: 0.6666 \t cross entropy: 0.009181235916912556\n","saving\n","epoch 134 accumulated train accuracy: 0.99386\n","[epoch 135, iteration 0] \t accuracy: 0.6605 \t cross entropy: 0.009020240977406502\n","epoch 135 accumulated train accuracy: 0.9936\n","[epoch 136, iteration 0] \t accuracy: 0.6625 \t cross entropy: 0.009065210819244385\n","epoch 136 accumulated train accuracy: 0.99334\n","[epoch 137, iteration 0] \t accuracy: 0.6586 \t cross entropy: 0.009174340404570103\n","epoch 137 accumulated train accuracy: 0.99494\n","[epoch 138, iteration 0] \t accuracy: 0.6677 \t cross entropy: 0.009088881313800812\n","saving\n","epoch 138 accumulated train accuracy: 0.9942\n","[epoch 139, iteration 0] \t accuracy: 0.6616 \t cross entropy: 0.009171182289719582\n","epoch 139 accumulated train accuracy: 0.99378\n","[epoch 140, iteration 0] \t accuracy: 0.6599 \t cross entropy: 0.009156775660812855\n","epoch 140 accumulated train accuracy: 0.99446\n","[epoch 141, iteration 0] \t accuracy: 0.6645 \t cross entropy: 0.008929619565606117\n","epoch 141 accumulated train accuracy: 0.99494\n","[epoch 142, iteration 0] \t accuracy: 0.6661 \t cross entropy: 0.008994638919830322\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-196d97196c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_cross_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                     \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Bb80XjAKwG_F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"2d68d216-ad53-4125-f447-024993e45ffc","executionInfo":{"status":"ok","timestamp":1590610131991,"user_tz":-300,"elapsed":1201,"user":{"displayName":"Muzaffar Soliyev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDiwg3y3NRPV9nPndG4HhHsJLtwkiGFGA3Jlr=s64","userId":"14865452875832345501"}}},"source":["plt.title('Batch normalization loss')\n","#plt.plot(train_cross_entropy)\n","plt.plot(validation_cross_entropy)\n","plt.show()\n"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"-ZDtEfsXroe-","colab_type":"code","colab":{}},"source":["with open(\"train_cross_entropy.txt\",'w') as f:\n","  for i in train_cross_entropy: \n","    f.write(str(i) + '\\n')\n","\n","with open(\"validation_cross_entropy.txt\",'w') as f:\n","  for i in validation_cross_entropy: \n","    f.write(str(i) + '\\n')\n","\n","with open(\"train_accuracy.txt\",'w') as f:\n","  for i in train_accuracy:\n","    f.write(str(i) + '\\n')\n","\n","with open(\"validation_accuracy.txt\",'w') as f:\n","  for i in validation_accuracy:\n","    f.write(str(i) + '\\n')\n"],"execution_count":0,"outputs":[]}]}